{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created by: Samuel Edet\n",
    "Reference: Python Machine Learning by Sebastian Raschka \n",
    "Date: 12th May, 2018.\n",
    "Title: Regression analysis on Housing price dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the Housing Dataset, which contains information about houses in the suburbs of Boston collected by D. Harrison and D.L. Rubinfeld in 1978. You can downlaod the dataset onb Kaggle.\n",
    "\n",
    "The features of the 506 samples may be summarized as shown in the excerpt of the\n",
    "dataset description:\n",
    "\n",
    "• CRIM: This is the per capita crime rate by town. \n",
    "\n",
    "• ZN: This is the proportion of residential land zoned for lots larger than\n",
    "25,000 sq.ft. \n",
    "\n",
    "• INDUS: This is the proportion of non-retail business acres per town. \n",
    "\n",
    "• CHAS: This is the Charles River dummy variable (this is equal to 1 if tract\n",
    "bounds river; 0 otherwise).\n",
    "\n",
    "• NOX: This is the nitric oxides concentration (parts per 10 million).\n",
    "\n",
    "• RM: This is the average number of rooms per dwelling.\n",
    "\n",
    "• AGE: This is the proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "• DIS: This is the weighted distances to five Boston employment centers.\n",
    "\n",
    "• RAD: This is the index of accessibility to radial highways.\n",
    "\n",
    "• TAX: This is the full-value property-tax rate per $\\$10,000$.\n",
    "\n",
    "• PTRATIO: This is the pupil-teacher ratio by town.\n",
    "\n",
    "• B: This is calculated as 1000(Bk - 0.63)^2, where Bk is the proportion of\n",
    "people of African American descent by town.\n",
    "\n",
    "• LSTAT: This is the percentage lower status of the population.\n",
    "\n",
    "• MEDV: This is the median value of owner-occupied homes in $\\$ 1000s$.\n",
    "\n",
    "\n",
    "MEDV is considered the target(dependent) variable, while the others are explanatory (independent) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fetching Dataset From UCI Repository\n",
    "#import pandas as pd\n",
    "#df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learningdatabases/housing/housing.data',header=None, sep='\\s+')\n",
    "#df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS','NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "#df.head()\n",
    "\n",
    "#Observation: It appears the dataset is no longer on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset from the computer\n",
    "import pandas as pd\n",
    "df = pd.read_csv('housingdata.csv')\n",
    "df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS','NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a scatterplot matrix to visualize the pairwise correlations between the different features in the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "cols = ['CRIM','LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']\n",
    "sns.pairplot(df[cols], size=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the scatterplot, we observe the following:\n",
    "    \n",
    "1. There seems to be a positive linear relationship between the average number of rooms per dwelling(RM) \n",
    "and the median value of owner occupied homes (MEDV). \n",
    "\n",
    "2. There seems to be an almost negative linear relationship between the percentage lower status of the population(LSTAT) \n",
    "and the median value of owner occupied homes (MEDV). \n",
    "\n",
    "3. The median value of owner occupied homes (MEDV) is normally distributed with several outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To quantify the linear realtionship between the features, we create a correlation matrix. \n",
    "#First, we use the NumPy's corrcoef function, then the seaborn's heatmap function \n",
    "#will be used to plot the correlation matrix array as a heat map:\n",
    "\n",
    "import numpy as np\n",
    "cm = np.corrcoef(df[cols].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=False,fmt='.2f',annot_kws={'size': 15},yticklabels=cols,xticklabels=cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimating the coefficient of a regression model via scikitlearn.\n",
    "# Scikit learn makes use of the LIBLINEAR library and advanced optimization algorithms \n",
    "#that work better with unstandardized variables.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X=df[['RM']].values\n",
    "y=df['MEDV'].values\n",
    "slr = LinearRegression()\n",
    "slr.fit(X, y)\n",
    "print('Slope: %.3f' % slr.coef_[0], 'Intercept: %.3f' % slr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting robust regression model using RANSAC\n",
    "#An alternative way to throwing out outliers is using the RANdom SAmple Consensus (RANSAC) algorithm, which\n",
    "#fits a regression model to a subset of the data, the so-called inliers.\n",
    "\n",
    "'''\n",
    "Summary of the iterative RANSAC algorithm as follows:\n",
    "\n",
    "1. Select a random number of samples to be inliers and fit the model.\n",
    "\n",
    "2. Test all other data points against the fitted model and add those points\n",
    "that fall within a user-given tolerance to the inliers.\n",
    "\n",
    "3. Refit the model using all inliers.\n",
    "\n",
    "4. Estimate the error of the fitted model versus the inliers.\n",
    "\n",
    "5. Terminate the algorithm if the performance meets a certain user-defined\n",
    "threshold or if a fixed number of iterations has been reached; go back to\n",
    "step 1 otherwise\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "ransac = RANSACRegressor(LinearRegression(),max_trials=100,min_samples=50,residual_metric=lambda x: np.sum(np.abs(x), axis=1),\n",
    "                         residual_threshold=5.0,random_state=0)\n",
    "ransac.fit(X, y)\n",
    "\n",
    "'''\n",
    "NOTE: \n",
    "Using the residual_metric parameter, we provided a callable lambda\n",
    "function that simply calculates the absolute vertical distances between the fitted line\n",
    "and the sample points. By setting the residual_threshold parameter to 5.0, we\n",
    "only allowed samples to be included in the inlier set if their vertical distance to the\n",
    "fitted line is within 5 distance units (this is discretional).\n",
    "\n",
    "By default, scikit-learn uses the MAD estimate to select the inlier threshold, where MAD\n",
    "stands for the Median Absolute Deviation of the target values y. However, the choice\n",
    "of an appropriate value for the inlier threshold is problem-specific, which is one\n",
    "disadvantage of RANSAC. Many different approaches have been developed over the\n",
    "recent years to select a good inlier threshold automatically. You can find a detailed\n",
    "discussion in R. Toldo and A. Fusiello's. Automatic Estimation of the Inlier Threshold in\n",
    "Robust Multiple Structures Fitting (in Image Analysis and Processing–ICIAP 2009,\n",
    "pages 123–131. Springer, 2009)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "After we have fitted the RANSAC model, let's obtain the inliers and outliers from the\n",
    "fitted RANSAC linear regression model and plot them together with the linear fit:\n",
    "'''\n",
    "inlier_mask = ransac.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "line_X = np.arange(5, 10, 1)\n",
    "line_y_ransac = ransac.predict(line_X[:, np.newaxis])\n",
    "plt.scatter(X[inlier_mask], y[inlier_mask],c='blue', marker='o', label='Inliers')\n",
    "plt.scatter(X[outlier_mask], y[outlier_mask],c='lightgreen', marker='s', label='Outliers')\n",
    "plt.plot(line_X, line_y_ransac, color='red')\n",
    "plt.xlabel('Average number of rooms [RM]')\n",
    "plt.ylabel('Price in $1000\\'s [MEDV]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the regression line is fitted around the inliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "When we print the slope and intercept of the model executing the following code,\n",
    "we can see that the linear regression line is slightly different from the fit that we\n",
    "obtained in the previous section without RANSAC:\n",
    "\n",
    "This reduces the potential effect of outliers\n",
    "'''\n",
    "print('Slope: %.3f' % ransac.estimator_.coef_[0], 'Intercept: %.3f' % ransac.estimator_.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Performance of Linear Regression Model\n",
    "\n",
    "'''\n",
    "1.RESIDUAL PLOTS: \n",
    "These are a commonly used graphical analysis for diagnosing\n",
    "regression models to detect nonlinearity and outliers, and to check if the errors\n",
    "are randomly distributed.\n",
    "\n",
    "Now we will run a linear regression on all the explanatory variables.\n",
    "'''\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['MEDV'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "slr = LinearRegression()\n",
    "slr.fit(X_train, y_train)\n",
    "y_train_pred = slr.predict(X_train)\n",
    "y_test_pred = slr.predict(X_test)\n",
    "\n",
    "\n",
    "#Using the following code, we will now plot a residual plot where we simply subtract\n",
    "#the true target variables from our predicted responses:\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train,c='blue', marker='o', label='Training data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test,c='lightgreen', marker='s', label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')\n",
    "plt.xlim([-10, 50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a perfect prediction, the residuals would be exactly zero, which we will probably never encounter in realistic and practical applications. However, for a good regression model, we would expect that the errors are randomly distributed and\n",
    "the residuals should be randomly scattered around the centerline. If we see patterns in a residual plot, it means that our model is unable to capture some explanatory information, which is leaked into the residuals as we can slightly see in our preceding\n",
    "residual plot. Furthermore, we can also use residual plots to detect outliers, which are represented by the points with a large deviation from the centerline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. MSE and Coefficient of Determination (R^2):\n",
    "Sometimes it may be more useful to report the coefficient of determination ( R^2 ), which can be understood as a \n",
    "standardized version of the mean square error (MSE), for better interpretability of the model performance. \n",
    "In other words, R^2 is the fraction of response variance that is captured by the model\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print('MSE train: %.3f, MSE test: %.3f' % (mean_squared_error(y_train, y_train_pred),mean_squared_error(y_test, y_test_pred)))\n",
    "\n",
    "print('R^2 train: %.3f, R^2 test: %.3f' % (r2_score(y_train, y_train_pred),r2_score(y_test, y_test_pred)))                                                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the MSE on the training set is smaller than the MSE of the test set, then this is an indicator that the model is overfitting\n",
    "the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Example on Polynomial Regression (Modeling non-linear relationships between LSTAT and MEDV)\n",
    "'''\n",
    "Although nonlinear relationships between variables can be modeled as multiple linear regression\n",
    "if they are linear in the parameters, we can also model them using polynomial regression.\n",
    "\n",
    "By executing the following code, we will model the relationship between house prices (MEDV) \n",
    "and LSTAT (percent lower status of the population) using second degree (quadratic) and \n",
    "third degree (cubic) polynomials and compare it to a linear fit.\n",
    "'''\n",
    "X = df[['LSTAT']].values\n",
    "y = df['MEDV'].values\n",
    "regr = LinearRegression()\n",
    "\n",
    "# create polynomial features\n",
    "quadratic = PolynomialFeatures(degree=2)\n",
    "cubic = PolynomialFeatures(degree=3)\n",
    "X_quad = quadratic.fit_transform(X)\n",
    "X_cubic = cubic.fit_transform(X)\n",
    "\n",
    "# linear fit\n",
    "X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]\n",
    "regr = regr.fit(X, y)\n",
    "y_lin_fit = regr.predict(X_fit)\n",
    "linear_r2 = r2_score(y, regr.predict(X))\n",
    "\n",
    "# quadratic fit\n",
    "regr = regr.fit(X_quad, y)\n",
    "y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))\n",
    "quadratic_r2 = r2_score(y, regr.predict(X_quad))\n",
    "\n",
    "# cubic fit\n",
    "regr = regr.fit(X_cubic, y)\n",
    "y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))\n",
    "cubic_r2 = r2_score(y, regr.predict(X_cubic))\n",
    "\n",
    "\n",
    "#Plot the results:\n",
    "plt.scatter(X, y,label='training points',color='lightgray')\n",
    "\n",
    "plt.plot(X_fit, y_lin_fit,label='linear (d=1), $R^2=%.2f$'% linear_r2,color='blue',lw=2,linestyle=':')\n",
    "\n",
    "plt.plot(X_fit, y_quad_fit,label='quadratic (d=2), $R^2=%.2f$' % quadratic_r2,color='red',lw=2,linestyle='-')\n",
    "\n",
    "plt.plot(X_fit, y_cubic_fit,label='cubic (d=3), $R^2=%.2f$'% cubic_r2,color='green',lw=2,linestyle='--')\n",
    "\n",
    "plt.xlabel('% lower status of the population [LSTAT]')\n",
    "plt.ylabel('Price in $1000\\'s [MEDV]')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the non linear relationship between LSTAT and MEDV is best captured by the cubic fit; and the coefficient of determination is highest (0.66)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The polynomial regression model may not be a best choice.By observing the scatterplot of MEDV and LSTAT, one can hypothesize \n",
    "that a log transformation of the LSTAT feature variable and the square root of the MEDV may \n",
    "project the data onto a linear feature space suitable for a linear regression fit.\n",
    "'''\n",
    "\n",
    "# transform features\n",
    "X_log = np.log(X)\n",
    "y_sqrt = np.sqrt(y)\n",
    "\n",
    "# fit features\n",
    "X_fit = np.arange(X_log.min()-1,X_log.max()+1, 1)[:, np.newaxis]\n",
    "regr = regr.fit(X_log, y_sqrt)\n",
    "y_lin_fit = regr.predict(X_fit)\n",
    "linear_r2 = r2_score(y_sqrt, regr.predict(X_log))\n",
    "\n",
    "# plot results\n",
    "plt.scatter(X_log, y_sqrt,label='training points', color='lightgray')\n",
    "\n",
    "plt.plot(X_fit, y_lin_fit,label='linear (d=1), $R^2=%.2f$' % linear_r2, color='blue',lw=2)\n",
    "plt.xlabel('log(% lower status of the population [LSTAT])')\n",
    "plt.ylabel('$\\sqrt{Price \\; in \\; \\$1000\\'s [MEDV]}$')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This current linear model is a better choice than the previous polynomial model, since the coefficient of determination (0.69) is better than that of the polynomial model (0.66)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
